{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa29ebc",
   "metadata": {},
   "source": [
    "# Trabalho A1 - Tiago Barradas - 243902\n",
    "\n",
    "## Part 1 - Data loading and pre-processing\n",
    "\n",
    "Primeiramente, vamos baixar o dataset e começar a explorá-lo em busca de problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95e4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=11)\n",
    "mpl.rc('xtick', labelsize=13)\n",
    "mpl.rc('ytick', labelsize=13)\n",
    "mpl.rc('grid', alpha=0)\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "# Baixa o dataset\n",
    "URL = \"https://raw.githubusercontent.com/tidyverse/ggplot2/main/data-raw/diamonds.csv\"\n",
    "DATASET_PATH = os.path.join(\"dataset\", \"diamonds.csv\")\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "urllib.request.urlretrieve(URL, DATASET_PATH)\n",
    "\n",
    "diamonds = pd.read_csv(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f3ae9",
   "metadata": {},
   "source": [
    "A princípo, não há nenhum datapoint com features faltantes, o que é um ótimo começo. Vamos agora checar se há datapoints repetidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e2098ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    53794\n",
       "True       146\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diamonds.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48344e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 53794 entries, 0 to 53939\n",
      "Data columns (total 10 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   carat    53794 non-null  float64\n",
      " 1   cut      53794 non-null  object \n",
      " 2   color    53794 non-null  object \n",
      " 3   clarity  53794 non-null  object \n",
      " 4   depth    53794 non-null  float64\n",
      " 5   table    53794 non-null  float64\n",
      " 6   price    53794 non-null  int64  \n",
      " 7   x        53794 non-null  float64\n",
      " 8   y        53794 non-null  float64\n",
      " 9   z        53794 non-null  float64\n",
      "dtypes: float64(6), int64(1), object(3)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "diamonds = diamonds[~diamonds.duplicated()]\n",
    "diamonds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a6f478",
   "metadata": {},
   "source": [
    "Após remover os datapoints repetidos usando o método .duplicated(), temos um dataset com 53794 entradas. Para evitar quaisquer problemas com índices faltantes, irei resetar os índices para que eles possuam um range adequado. Depois disso, vamos checar as features \"cut\", \"color\", e \"clarity\" para nos certificar que todos possuem valores adequados, conforme a [referência indicada](https://ggplot2.tidyverse.org/reference/diamonds.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4c9af2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 53794 entries, 0 to 53793\n",
      "Data columns (total 10 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   carat    53794 non-null  float64\n",
      " 1   cut      53794 non-null  object \n",
      " 2   color    53794 non-null  object \n",
      " 3   clarity  53794 non-null  object \n",
      " 4   depth    53794 non-null  float64\n",
      " 5   table    53794 non-null  float64\n",
      " 6   price    53794 non-null  int64  \n",
      " 7   x        53794 non-null  float64\n",
      " 8   y        53794 non-null  float64\n",
      " 9   z        53794 non-null  float64\n",
      "dtypes: float64(6), int64(1), object(3)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "diamonds.reset_index(inplace=True, drop=True)\n",
    "diamonds.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26f4a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds[\"cut\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96fc152",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds[\"color\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a485c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds[\"clarity\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136ba4b",
   "metadata": {},
   "source": [
    "Agora que nos certificamos que o dataset está limpo, vamos trabalhar nos histogramas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431645db",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = diamonds[\"cut\"].value_counts().index \n",
    "\n",
    "\n",
    "for key in diamonds.keys():\n",
    "    fig, axs = plt.subplots(2, figsize=(11, 10))\n",
    "    fig.suptitle(f\"Histograma de {key}\", fontsize=24)\n",
    "    sns.histplot(ax=axs[0], data=diamonds, x=diamonds[key], \n",
    "                 stat='density', bins=50, common_norm=True)\n",
    "\n",
    "    sns.histplot(ax=axs[1], data=diamonds, x=diamonds[key], \n",
    "                 stat='density', bins=50, common_norm=True, hue=\"cut\")\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581e52cb",
   "metadata": {},
   "source": [
    "Com os histogramas feitos, vamos agora fazer nossa matriz de figuras, contendo histogramas e scatterplots que mostram a correlação entre as diversas features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(x, y, **kws):\n",
    "    corr = np.corrcoef(diamonds[x.name], diamonds[y.name])\n",
    "    ax = plt.gca()  # Get current axis from the whole figure\n",
    "    ax.annotate(f\"Correlação: {round(corr[0, 1], 2)}\", \n",
    "                xy=(0.25, 0.45), xycoords=ax.transAxes)\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "plot_matrix = sns.PairGrid(diamonds, hue='cut', diag_sharey=False)\n",
    "plot_matrix.map_diag(sns.histplot, stat='density', bins=50, common_norm=True)\n",
    "plot_matrix.map_lower(sns.scatterplot)\n",
    "plot_matrix.map_upper(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cbf14e",
   "metadata": {},
   "source": [
    "Com isso, a primeira parte está concluída!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f78f34a",
   "metadata": {},
   "source": [
    "## Part 2 - K-nearest-neighbour classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b819cd4",
   "metadata": {},
   "source": [
    "Vamos dividir nosso dataset em training, validation e testing sets, e normalizar os valores presentes em cada feature do dataset para que elas passem a compartilhar uma mesma escala. Para isso, é necessário primeiro remover as variáveis categóricas do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633aaa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "X_diamonds = diamonds.drop([\"cut\", \"color\", \"clarity\"], axis=1)\n",
    "y_diamonds = diamonds[\"cut\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_diamonds, y_diamonds, test_size=0.2, random_state=42, stratify=y_diamonds)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.2, random_state=42, stratify=y_test) \n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_train[X_train.columns] = min_max_scaler.fit_transform(X_train)  # Usando essa sintaxe para ter retorno como DataFrame\n",
    "X_test[X_test.columns] = min_max_scaler.transform(X_test)\n",
    "X_val[X_val.columns] = min_max_scaler.transform(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ebed10",
   "metadata": {},
   "source": [
    "Agora, vamos implementar nosso próprio estimator para realizar a classificação K-Nearest-Neighbours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f316680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class KNearestNeighbours(BaseEstimator):\n",
    "    def __init__(self, k=5, distances='euclidean'):\n",
    "        self.k = k  # Number of nearest neighbours to consider\n",
    "        self.distances = distances  # Can also be a matrix with precalculated distances\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X  # Training data (in a dataframe)\n",
    "        self.y_train = pd.DataFrame(y)  # Training labels (in a series that gets converted)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # X as array\n",
    "        labels = []\n",
    "\n",
    "        if self.distances=='euclidean':\n",
    "            # For each new datapoint, calculate and sort by the distance to each training point,\n",
    "            # and select the most common label in the k nearest neighbours\n",
    "            for datapoint in X:  \n",
    "                point_dist = np.linalg.norm(self.X_train - datapoint, axis=1)\n",
    "                sorted_dist = np.argsort(point_dist)[:self.k]\n",
    "                predicted_label = self.y_train.iloc[sorted_dist, :].value_counts().index[0][0] \n",
    "                labels.append(predicted_label)\n",
    "        else:\n",
    "            # For each new datapoint, sort the previously calculated distances and \n",
    "            # select the most common label in the k nearest neighbours\n",
    "            if self.distances.shape[1] != self.X_train.shape[0]:\n",
    "                raise ValueError('Distances matrix must have the same number of columns as the number of datapoints in the training data')\n",
    "            for datapoint in self.distances:\n",
    "                sorted_dist = np.argsort(datapoint)[:self.k]\n",
    "                predicted_label = self.y_train.iloc[sorted_dist, :].value_counts().index[0][0] \n",
    "                labels.append(predicted_label)\n",
    "        return np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c8f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn_dim = KNearestNeighbours(k=3)\n",
    "knn_dim.fit(X_train, y_train)\n",
    "y_pred = knn_dim.predict(np.array(X_test))\n",
    "\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08057139",
   "metadata": {},
   "source": [
    "Por curiosidade, quero compará-la com o modelo nativo do Scikit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "skknn = KNeighborsClassifier(n_neighbors=3)\n",
    "skknn.fit(X_train, y_train)\n",
    "sky_pred = skknn.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test, sky_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f18972",
   "metadata": {},
   "source": [
    "Obtivemos o exato mesmo resultado! Isso indica que a matemática por trás do nosso próprio modelo está correta. A maior diferença entre essas duas implementações é a gigantesca eficiência de código presente na do SciKit-Learn (demorando menos de 1 segundo enquanto a minha implementação demora quase 1 minuto e meio!). Levando isso em conta, eu usarei essa implementação mais rápida para os próximos passos do trabalho, a fim de economizar horas de código rodando."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e37782",
   "metadata": {},
   "source": [
    "Para montar as 35 combinações de diferentes features, usarei a função combinations do itertools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6ee63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "feature_combs = np.array(list(combinations(X_diamonds.keys(), 3)))\n",
    "feature_combs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20fa5b",
   "metadata": {},
   "source": [
    "Agora, implementarei uma classe Transformer do scikitlearn, que recebe como hiperparâmetro uma array de índices de DataFrame, que será usado para filtrar o DataFrame recebido conforme as features selecionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4536361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Transformer that returns the filtered dataset based on the features given\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, features=None):\n",
    "        self.features = features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.features]\n",
    "\n",
    "# Pipeline to be used on the GridSearch \n",
    "# (first filters the features, which gets fitted to the model)\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('selector', FeatureSelector()),\n",
    "        ('knn', KNeighborsClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set of hyperparameter combinations to try out\n",
    "param_grid = {'knn__n_neighbors': [2, 5, 10, 15], \n",
    "              'selector__features': feature_combs.tolist()}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=4, \n",
    "                           scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "accuracy_score(grid_search.predict(X_val), y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d5e1c",
   "metadata": {},
   "source": [
    "É possível converter os dados resultantes das testagens do GridSearch para um dataframe Pandas. Nesse caso, os dados que nos interessam são as features, o valor de k, e a accuracy do modelo gerado por essa combinação. Podemos montar esse dataframe da seguinte forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(grid_search.cv_results_[\"params\"]),\n",
    "        pd.DataFrame(grid_search.cv_results_[\"mean_test_score\"], columns=[\"Accuracy\"])\n",
    "    ], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "accuracies = accuracies.astype({\"selector__features\": str})\n",
    "accuracies.sort_values(by=\"Accuracy\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28509bb7",
   "metadata": {},
   "source": [
    "Como podemos ver, os modelos mais precisos para prever a feature \"cut\" são os com os maiores números de neighbors e que se baseiam, principalmente, nas features \"depth\" e \"table\". Agora, vamos implementar o código da função de plotagem desses dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFeatureByCategories(dataframe, feature, category):   \n",
    "    list_of_ks = dataframe[category].unique()  # Gets the unique categories\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=list_of_ks.size, figsize=(18, 12))  # Determines the amount of subplots based on the categories\n",
    "\n",
    "    is_first = True  \n",
    "    for item, ax in zip(list_of_ks, axs.ravel()):  # Iterates through the axs and categories\n",
    "        dataframe_aux = dataframe[dataframe[category] == item]  # Filters the dataframe based on the category\n",
    "\n",
    "        colors = ['steelblue' if (x < max(dataframe_aux[feature])) else 'firebrick' for x in dataframe_aux[feature]]  # Colors the highest value in red\n",
    "\n",
    "        sns.barplot(ax=ax, data=dataframe_aux, x=feature, y='selector__features', palette=colors)\n",
    "        ax.set_title(f'k = {item}')\n",
    "        ax.set(xlim=(0, 1))\n",
    "        ax.set_ylabel('Features')\n",
    "\n",
    "        if not is_first:  # Removes the labels if it's not the first plot\n",
    "            ax.set_ylabel('')\n",
    "            ax.set_yticklabels('', visible=False)\n",
    "        is_first = False\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "\n",
    "plotFeatureByCategories(accuracies, 'Accuracy', 'knn__n_neighbors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12828281",
   "metadata": {},
   "source": [
    "O gráfico mostra claramente que a combinação \"depth, table, x\" é a mais vantajosa independentemente dos valores de k checados. Com isso, a parte 2 do trabalho está concluída!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cec520",
   "metadata": {},
   "source": [
    "## Part 3 - Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd2dc8",
   "metadata": {},
   "source": [
    "Primeiro, vamos redefinir os training, testing e validation sets para se adequar ao fato de que agora iremos tentar prever os valores de preço:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6522aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_lr = X_train['price']\n",
    "X_train_lr = X_train.drop(['price'], axis=1)\n",
    "\n",
    "y_test_lr = X_test['price']\n",
    "X_test_lr = X_test.drop(['price'], axis=1)\n",
    "\n",
    "y_val_lr = X_val['price']\n",
    "X_val_lr = X_val.drop(['price'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fba6aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearRegressionSolver(BaseEstimator):\n",
    "    def __init__(self, solver='cf', max_iter=100, learning_rate=0.0001, weight_decay=0.1):\n",
    "        self.solver = solver \n",
    "        self.weight_decay = weight_decay\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.y_train = y\n",
    "\n",
    "        if self.solver == 'cf':  # Closed form solution\n",
    "            A = np.eye(X.shape[1]+1)\n",
    "            A[0, 0] = 0\n",
    "            L2_factor = A*self.weight_decay\n",
    "            inverse = np.linalg.inv(self.X_train.T @ self.X_train + L2_factor)\n",
    "            self.weights = inverse @ self.X_train.T @ self.y_train\n",
    "        \n",
    "\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.c_[np.ones((X.shape[0], 1)), X] @ self.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a68d30e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08180214495237242"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegressionSolver(solver='cf', weight_decay=0.1)\n",
    "lr.fit(X_train_lr[:10000], y_train_lr[:10000])\n",
    "y_pred_lr = lr.predict(X_test_lr)\n",
    "\n",
    "np.sqrt(mean_squared_error(y_test_lr, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7716f131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0818021449523721"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge_reg = Ridge(alpha=0.1)\n",
    "ridge_reg.fit(X_train_lr[:10000], y_train_lr[:10000])\n",
    "y_pred_ridge = ridge_reg.predict(X_test_lr)\n",
    "\n",
    "np.sqrt(mean_squared_error(y_test_lr, y_pred_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c2826c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.eye(5)\n",
    "A[0, 0] = 0"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "28ee51784dd090c47dc144afaec394fd98f22579fbe58b047cabf5cb0e631e08"
  },
  "kernelspec": {
   "display_name": "TACD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
